# -*- coding: utf-8 -*-
"""Patient_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CFiArA_NW0V78le2O4cFSdvkVbGhxrlY
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('filtered_df.csv')
df.head()

df.describe()

df.info()

"""# EDA and DATA VISUALIZATION"""

# Missing values
df.isnull().sum()

df.duplicated().sum()

len(df['drugName'].unique())

# Most popular drugs
# Top 20
df['drugName'].value_counts().nlargest(20)

# Top 20 Drugs(MOst Popular)
plt.figure(figsize=(20,10))
df['drugName'].value_counts().nlargest(20).plot(kind='bar')
plt.title('Top 20 Most popular drugs based on counts')
plt.show()

# Grouping the data by rating and count the frequency of each rating
rating_counts = df.groupby('rating').size().reset_index(name='counts')

# Sort the data by the frequency of each rating
rating_counts = rating_counts.sort_values(by='counts',ascending=False)

# Plot a bar chart of the 20 most popular ratings
plt.figure(figsize=(15,10))
rating_counts[:20].plot.bar(x='rating',y='counts',color='blue')
plt.xlabel('Rating')
plt.ylabel('Counts')
plt.title('20 most popular ratings')
plt.show()

# Plot a pie chart of the distribution of ratings
plt.figure(figsize=(15,10))
plt.pie(rating_counts['counts'], labels=rating_counts['rating'], startangle=90, counterclock=False, autopct='%1.1f%%')
plt.axis('equal')
plt.title('Distribution of Ratings')
plt.show()

# Get the 20 most popular drugs based on usefulCount
plt.figure(figsize=(20,10))
top_20_drugs = df.groupby('drugName')['usefulCount'].sum().sort_values(ascending=False).head(20)

# Plot the bar plot
top_20_drugs.plot(kind='bar', color='blue')
plt.xlabel('Drug Name')
plt.ylabel('Useful Count')
plt.title('Top 20 Drugs based on Useful Count')
plt.xticks(rotation=90)
plt.show()

from wordcloud import WordCloud
from wordcloud import STOPWORDS

stopwords = set(STOPWORDS)

wordcloud = WordCloud(stopwords = stopwords, width = 1200, height = 800).generate(str(df['drugName']))

plt.rcParams['figure.figsize'] = (17, 17)
plt.title('Word Cloud - Drug Names', fontsize = 25)
print(wordcloud)
plt.axis('off')
plt.imshow(wordcloud)
plt.show()

# Convert the date column to a datetime format
df['date'] = pd.to_datetime(df['date'])

# Group the data by date and count the number of reviews for each date
reviews_per_date = df.groupby(df['date'].dt.date).size().reset_index(name='counts')

# Plot a line chart of the trend in the number of reviews over time
plt.plot(reviews_per_date['date'], reviews_per_date['counts'], color='blue')
plt.xlabel('Date')
plt.ylabel('Number of Reviews')
plt.title('Trend in the Number of Reviews Over Time')
plt.show()

"""Interpretation:

1-Improved Accessibility: There might have been improvements in the accessibility of the drug, either through increased availability or decreased cost, which led to an increase in the number of patients using the drug and writing reviews.

2-Increased Use of Online Platforms: The increase in the use of online platforms for reviewing drugs and conditions might have contributed to the increase in the number of reviews from 2015 onwards.

3-Changes in Marketing Strategies: The drug manufacturer or the healthcare industry might have changed their marketing strategies to increase the visibility of the drug, which led to an increase in the number of patients using the drug and writing reviews.

# Feature Engineering/Data preprocessing
"""

import nltk
import string
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

def clean_review(review):
    # Convert to lowercase
    review = review.lower()
    
    # Remove punctuation
    review = review.translate(str.maketrans('', '', string.punctuation))
    
    # Tokenize the review
    tokens = nltk.word_tokenize(review)
    
    # Remove stop words
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    
    # Join the cleaned tokens back together
    cleaned_review = ' '.join(tokens)
    
    return cleaned_review

df['review'] = df['review'].apply(lambda x: clean_review(x))

"""The importance of the date feature depends on the business problem and the type of model being used. In some cases, the date feature may be important in identifying patterns and trends over time. For example, if you are building a time-series model, the date feature would be an important input for the model. However, in other cases, the date feature may not be as relevant. For instance, if you are building a model to predict the rating based on the review, the date feature may not have a significant impact on the model's predictions."""

df.drop(['date'],axis=1,inplace=True)

# Get the stopwords as a list
stopwords = list(stopwords.words('english'))

# Join all the reviews into a single string
reviews_text = " ".join(review for review in df.review)

# Generate the wordcloud
wordcloud = WordCloud(width=800, height=800,
                      background_color='white',
                      stopwords=stopwords,
                      min_font_size=10).generate(reviews_text)

# Plot the wordcloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)

df

"""## MODEL BUILDING"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,mean_squared_error

df["rating"] = df["rating"].astype(int)

# Create the feature matrix
vectorizer = TfidfVectorizer(lowercase=True, stop_words="english")
reviews = vectorizer.fit_transform(df["review"])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(reviews, df["condition"], test_size=0.25, random_state=42)

"""## LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression

# Train the model
model = LogisticRegression(multi_class="ovr")
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Use the model to make predictions for new data
new_review = ["This drug is amazing, it really helped me with my condition!"]
new_review = vectorizer.transform(new_review)
new_condition = model.predict(new_review)[0]
print("Predicted Condition:", new_condition)

# Recommend the top 5 drugs with the highest average rating for the condition
drug_ratings = df[df["condition"] == new_condition].groupby("drugName")["rating"].mean()
recommended_drugs = drug_ratings.nlargest(5).index.tolist()
print("Recommended Drugs based on ratings only:")
for i, drug in enumerate(recommended_drugs):
    print(i+1, drug)

"""## RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

model2 = RandomForestClassifier()
model2.fit(X_train, y_train)

y_pred = model2.predict(X_test)
print(classification_report(y_test, y_pred))

new_review = ["This drug is amazing, it really helped me with my condition!"]
new_review = vectorizer.transform(new_review)
new_condition = model2.predict(new_review)[0]
print("Predicted Condition:", new_condition)

drug_ratings = df[df["condition"] == new_condition].groupby("drugName")["rating"].mean()
recommended_drugs = drug_ratings.nlargest(5).index.tolist()
print("Recommended Drugs based on ratings only:")
for i, drug in enumerate(recommended_drugs):
    print(i+1, drug)

"""## SUPPORT VECTOR MACHINE"""

from sklearn.svm import SVC

# Train the model
model3 = SVC()
model3.fit(X_train, y_train)

# Evaluate the model
y_pred = model3.predict(X_test)
print(classification_report(y_test, y_pred))

# Use the model to make predictions for new data
new_review = ["This drug is amazing, it really helped me with my condition!"]
new_review = vectorizer.transform(new_review)
new_condition = model3.predict(new_review)[0]
print("Predicted Condition:", new_condition)

drug_ratings = df[df["condition"] == new_condition].groupby("drugName")["rating"].mean()
recommended_drugs = drug_ratings.nlargest(5).index.tolist()
print("Recommended Drugs based on ratings only:")
for i, drug in enumerate(recommended_drugs):
    print(i+1, drug)

"""## GRADIENT BOOSTING"""

from sklearn.ensemble import GradientBoostingClassifier

# Train the model
model4 = GradientBoostingClassifier()
model4.fit(X_train, y_train)

# Evaluate the model
y_pred = model4.predict(X_test)
print(classification_report(y_test, y_pred))

# Use the model to make predictions for new data
new_review = ["This drug is amazing, it really helped me with my condition!"]
new_review = vectorizer.transform(new_review)
new_condition = model4.predict(new_review)[0]
print("Predicted Condition:", new_condition)

drug_ratings = df[df["condition"] == new_condition].groupby("drugName")["rating"].mean()
recommended_drugs = drug_ratings.nlargest(5).index.tolist()
print("Recommended Drugs based on ratings only:")
for i, drug in enumerate(recommended_drugs):
    print(i+1, drug)

"""## DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier

# Train the model
model5 = DecisionTreeClassifier()
model5.fit(X_train, y_train)

# Evaluate the model
y_pred = model5.predict(X_test)
print(classification_report(y_test, y_pred))

# Use the model to make predictions for new data
new_review = ["This drug is amazing, it really helped me with my condition!"]
new_review = vectorizer.transform(new_review)
new_condition = model5.predict(new_review)[0]
print("Predicted Condition:", new_condition)

drug_ratings = df[df["condition"] == new_condition].groupby("drugName")["rating"].mean()
recommended_drugs = drug_ratings.nlargest(5).index.tolist()
print("Recommended Drugs based on ratings only:")
for i, drug in enumerate(recommended_drugs):
    print(i+1, drug)

"""Support Vector Machine Giving Highest Accuracy"""